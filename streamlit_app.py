import time
import streamlit as st
from openai import OpenAI

# APIã‚­ãƒ¼ã‚’ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰å–å¾—ã—ã¦åˆæœŸåŒ–
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if "busy_until" not in st.session_state:
    st.session_state.busy_until = 0

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼æ©Ÿèƒ½ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if "APP_PASSWORD" in st.secrets and not st.session_state.authenticated:
    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if password == st.secrets["APP_PASSWORD"]:
            st.session_state.authenticated = True
            st.experimental_rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")
    st.stop()

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
st.title("ğŸ¤ Whisperæ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒª")

audio = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["mp3","wav","m4a","mp4","webm","mpeg4"])
model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", ["whisper-1", "gpt-4o-mini-transcribe"])

def transcribe_once(file, model_name):
    now = time.time()
    # 1åˆ†ä»¥å†…ã¯ãƒ–ãƒ­ãƒƒã‚¯
    if now < st.session_state.busy_until:
        st.error("1åˆ†ä»¥å†…ã®é€£ç¶šå‘¼ã³å‡ºã—ã¯ç¦æ­¢ã§ã™ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return None
    st.session_state.busy_until = now + 60

    try:
        # gpt-4o-mini-transcribeãƒ¢ãƒ‡ãƒ«ã«ã¯jsonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€whisper-1ã«ã¯verbose_jsonã‚’ä½¿ç”¨
        if model_name == "gpt-4o-mini-transcribe":
            response_format = "json"
        else:
            response_format = "verbose_json"
            
        return client.audio.transcriptions.create(
            model=model_name,
            file=file,
            response_format=response_format,
            timestamp_granularities=["segment"] if response_format == "verbose_json" else None
        )
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None
        
if audio and st.button("æ–‡å­—èµ·ã“ã—é–‹å§‹"):
    with st.spinner("æ–‡å­—èµ·ã“ã—ä¸­â€¦"):
        result = transcribe_once(audio, model)
    
    if result:
        # å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦è¡¨ç¤ºæ–¹æ³•ã‚’å¤‰ãˆã‚‹
        if model == "gpt-4o-mini-transcribe":
            # jsonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆï¼ˆè¾æ›¸å‹ã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ï¼‰
            st.subheader("æ–‡å­—èµ·ã“ã—çµæœ")
            full_text = result["text"]
            st.text_area("å®Œå…¨ãªãƒ†ã‚­ã‚¹ãƒˆ", full_text, height=200)
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
            if "segments" in result:
                st.subheader("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                for segment in result["segments"]:
                    start = segment.get("start", 0)
                    end = segment.get("end", 0)
                    text = segment.get("text", "")
                    st.markdown(f"**[{start:.2f}ç§’ - {end:.2f}ç§’]** {text}")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.download_button("ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜", full_text, file_name="transcript.txt")
        else:
            # verbose_jsonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
            st.subheader("æ–‡å­—èµ·ã“ã—çµæœ")
            st.text_area("å®Œå…¨ãªãƒ†ã‚­ã‚¹ãƒˆ", result.text, height=200)
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
            st.subheader("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            for segment in result.segments:
                start = segment.start
                end = segment.end
                st.markdown(f"**[{start:.2f}ç§’ - {end:.2f}ç§’]** {segment.text}")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.download_button("ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜", result.text, file_name="transcript.txt")
